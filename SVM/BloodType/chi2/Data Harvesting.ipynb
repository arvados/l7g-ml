{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========Beginning work on blood_type_B_no_filter_no_augmentation_X.npz and blood_type_B_no_filter_no_augmentation_y.npy===========\n",
      "Extracting data for blood type B\n",
      "Using no data augmentation\n",
      "Removing columns with more than 10% missing data\n",
      "Beginning to one-hot encode data\n",
      "Using no filter\n",
      "(79, 12045846)\n",
      "Using no filter\n",
      "(79, 12255109)\n",
      "Using no filter\n",
      "(79, 12098607)\n",
      "Just created datasets for blood_type_B_no_filter_no_augmentation_X.npz and blood_type_B_no_filter_no_augmentation_y.npy\n",
      "\n",
      "===========Beginning work on blood_type_Rh_chi2_balanced_augmentation_X.npz and blood_type_Rh_chi2_balanced_augmentation_y.npy===========\n",
      "Extracting data for blood type Rh\n",
      "Using balanced data augmentation. Note that this only works when the larger class is up to 5x the size of the smaller one.\n",
      "Removing columns with more than 10% missing data\n",
      "Beginning to one-hot encode data\n",
      "Using chi2 filter\n",
      "Using chi2 filter\n",
      "Using chi2 filter\n",
      "Just created datasets for blood_type_Rh_chi2_balanced_augmentation_X.npz and blood_type_Rh_chi2_balanced_augmentation_y.npy\n",
      "\n",
      "===========Beginning work on blood_type_Rh_chi2_no_augmentation_X.npz and blood_type_Rh_chi2_no_augmentation_y.npy===========\n",
      "Extracting data for blood type Rh\n",
      "Using no data augmentation\n",
      "Removing columns with more than 10% missing data\n",
      "Beginning to one-hot encode data\n",
      "Using chi2 filter\n",
      "Using chi2 filter\n",
      "Using chi2 filter\n",
      "Just created datasets for blood_type_Rh_chi2_no_augmentation_X.npz and blood_type_Rh_chi2_no_augmentation_y.npy\n",
      "\n",
      "===========Beginning work on blood_type_A_chi2_no_augmentation_X.npz and blood_type_A_chi2_no_augmentation_y.npy===========\n",
      "Extracting data for blood type A\n",
      "Using no data augmentation\n",
      "Removing columns with more than 10% missing data\n",
      "Beginning to one-hot encode data\n",
      "Using chi2 filter\n",
      "Using chi2 filter\n",
      "Using chi2 filter\n",
      "Just created datasets for blood_type_A_chi2_no_augmentation_X.npz and blood_type_A_chi2_no_augmentation_y.npy\n",
      "\n",
      "===========Beginning work on blood_type_B_chi2_no_augmentation_X.npz and blood_type_B_chi2_no_augmentation_y.npy===========\n",
      "Extracting data for blood type B\n",
      "Using no data augmentation\n",
      "Removing columns with more than 10% missing data\n",
      "Beginning to one-hot encode data\n",
      "Using chi2 filter\n",
      "Using chi2 filter\n",
      "Using chi2 filter\n",
      "Just created datasets for blood_type_B_chi2_no_augmentation_X.npz and blood_type_B_chi2_no_augmentation_y.npy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Format is (X_filename, y_filename, blood type, filter type, augmentation type)\n",
    "#TODO: Fix the balanced augmentation so it works for *very* unbalanced classes. Right now it only works if the smaller class\n",
    "# is within a factor of 5 of the bigger class\n",
    "filenames = [(\"blood_type_B_no_filter_no_augmentation_X.npz\", \"blood_type_B_no_filter_no_augmentation_y.npy\", \"B\", None, None),\n",
    "             (\"blood_type_Rh_chi2_balanced_augmentation_X.npz\", \"blood_type_Rh_chi2_balanced_augmentation_y.npy\", \"Rh\", \"chi2\", \"balanced\"),\n",
    "             (\"blood_type_Rh_chi2_no_augmentation_X.npz\", \"blood_type_Rh_chi2_no_augmentation_y.npy\", \"Rh\", \"chi2\", None),\n",
    "             (\"blood_type_A_chi2_no_augmentation_X.npz\", \"blood_type_A_chi2_no_augmentation_y.npy\", \"A\", \"chi2\", None),\n",
    "             (\"blood_type_B_chi2_no_augmentation_X.npz\", \"blood_type_B_chi2_no_augmentation_y.npy\", \"B\", \"chi2\", None)]\n",
    "\n",
    "for X_filename, y_filename, blood_type, filter_type, augmentation_type in filenames:\n",
    "    print(\"===========Beginning work on %s and %s===========\" % (X_filename, y_filename))\n",
    "    sns.set()\n",
    "    # load data from untap\n",
    "    conn = sqlite3.connect('./untap.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT * FROM demographics')\n",
    "    rows = c.fetchall()\n",
    "    colnames = [i[0] for i in c.description]\n",
    "    data = pd.DataFrame(rows, columns=colnames)\n",
    "    conn.close()\n",
    "    dataBloodType = data[['human_id', 'blood_type']]\n",
    "    dataBloodType = dataBloodType.replace('', np.nan, inplace=False)\n",
    "    dataBloodType = dataBloodType.dropna(axis=0, how='any', inplace=False)\n",
    "\n",
    "    # Creating dummy variables for A, B and rh factor\n",
    "    dataBloodType['A'] = dataBloodType['blood_type'].str.contains('A',na=False).astype(int)\n",
    "    dataBloodType['B'] = dataBloodType['blood_type'].str.contains('B',na=False).astype(int)\n",
    "    dataBloodType['Rh'] = dataBloodType['blood_type'].str.contains('\\+',na=False).astype(int)\n",
    "\n",
    "    # function to retrieve a tile file from keep\n",
    "    tiled_data_dir = \"./\"\n",
    "    def get_file(name, np_file = True):\n",
    "        if np_file: \n",
    "            return np.load(os.path.join(tiled_data_dir, name))\n",
    "        else:\n",
    "            return open(os.path.join(tiled_data_dir, name), 'r')\n",
    "\n",
    "    Xtrain = np.load('./all.npy')\n",
    "    path_data = np.load('./all-info.npy')\n",
    "\n",
    "    Xtrain += 2\n",
    "    names_file = get_file(\"names.npy\", np_file = False)\n",
    "    names = []\n",
    "    for line in names_file:\n",
    "        names.append(line[45:54][:-1])\n",
    "\n",
    "    # Getting phenotypes for huIDs that have associated genotypes\n",
    "\n",
    "    results = [i.lower() for i in names]\n",
    "\n",
    "    df = pd.DataFrame(results,columns={'Sample'})\n",
    "    df['Number'] = df.index\n",
    "    dataBloodType = data[['human_id', 'blood_type']]\n",
    "    dataBloodType = dataBloodType.replace('', np.nan, inplace=False)\n",
    "    dataBloodType = dataBloodType.dropna(axis=0, how='any', inplace=False)\n",
    "\n",
    "    # Creating dummy variables for A, B and rh factor\n",
    "    dataBloodType['A'] = dataBloodType['blood_type'].str.contains('A',na=False).astype(int)\n",
    "    dataBloodType['B'] = dataBloodType['blood_type'].str.contains('B',na=False).astype(int)\n",
    "    dataBloodType['Rh'] = dataBloodType['blood_type'].str.contains('\\+',na=False).astype(int)\n",
    "\n",
    "    dataBloodType.human_id = dataBloodType.human_id.str.lower()\n",
    "    df2 = df.merge(dataBloodType,left_on = 'Sample', right_on='human_id', how='inner')\n",
    "    del dataBloodType\n",
    "    df2['blood_type'].value_counts()\n",
    "    del df\n",
    "    idx = df2['Number'].values\n",
    "\n",
    "    Xtrain = Xtrain[idx,:] \n",
    "\n",
    "    # Remove tiles (columns) that don't have more than 1 tile varient at every position\n",
    "    # Actually probably will want to technically do this before the one-hot, so I am keeping these in for the moment\n",
    "\n",
    "    min_indicator = np.amin(Xtrain, axis=0)\n",
    "    max_indicator = np.amax(Xtrain, axis=0)\n",
    "\n",
    "    sameTile = min_indicator == max_indicator\n",
    "    skipTile = ~sameTile\n",
    "\n",
    "    idxOP = np.arange(Xtrain.shape[1])\n",
    "    Xtrain = Xtrain[:, skipTile]\n",
    "    newPaths = path_data[skipTile]\n",
    "    idxOP = idxOP[skipTile]\n",
    "\n",
    "    # only keep data with less than 10% missing data\n",
    "    nnz = np.count_nonzero(Xtrain, axis=0)\n",
    "    fracnnz = np.divide(nnz.astype(float), Xtrain.shape[0])\n",
    "\n",
    "    idxKeep = fracnnz >= 0.9\n",
    "    Xtrain = Xtrain[:, idxKeep]\n",
    "    \n",
    "    if blood_type == \"A\":\n",
    "        y = df2.A.values\n",
    "        print(\"Extracting data for blood type A\")\n",
    "    elif blood_type == \"B\":\n",
    "        y = df2.B.values\n",
    "        print(\"Extracting data for blood type B\")\n",
    "    elif blood_type == \"Rh\":\n",
    "        y = df2.Rh.values\n",
    "        print(\"Extracting data for blood type Rh\")\n",
    "    else:\n",
    "        raise ValueError(\"The blood type specified isn't currently handled. You may need to edit the code block around this error to handle it.\")\n",
    "    \n",
    "    # save information about deleting missing/spanning data\n",
    "    varvals = np.full(50 * Xtrain.shape[1], np.nan)\n",
    "    nx = 0\n",
    "\n",
    "    varlist = []\n",
    "    for j in range(0, Xtrain.shape[1]):\n",
    "        u = np.unique(Xtrain[:,j])\n",
    "        varvals[nx : nx + u.size] = u\n",
    "        nx = nx + u.size\n",
    "        varlist.append(u)\n",
    "\n",
    "    varvals = varvals[~np.isnan(varvals)]\n",
    "    np.save(\"./varvals.npy\", varvals)\n",
    "\n",
    "    def foo(col):\n",
    "        u = np.unique(col)\n",
    "        nunq = u.shape\n",
    "        return nunq\n",
    "\n",
    "    invals = np.apply_along_axis(foo, 0, Xtrain)\n",
    "    invals = invals[0]\n",
    "\n",
    "    # used later to find coefPaths\n",
    "    pathdataOH = np.repeat(newPaths[idxKeep], invals)\n",
    "    # used later to find the original location of the path from non one hot\n",
    "    oldpath = np.repeat(idxOP[idxKeep], invals)\n",
    "    np.save(\"./idx_keep.npy\", idxKeep)\n",
    "    np.save(\"./path_data_oh.npy\", pathdataOH)\n",
    "    np.save(\"./old_path.npy\", oldpath)\n",
    "    np.save(\"./train_data.npy\", Xtrain)\n",
    "    np.save(\"./blood_types.npy\", y)\n",
    "    np.save('./path_data.npy', newPaths)\n",
    "    \n",
    "    train_data = np.load(\"./train_data.npy\")\n",
    "    blood_types = np.load(\"./blood_types.npy\")\n",
    "    if augmentation_type == \"balanced\":\n",
    "        print(\"Using balanced data augmentation. Note that this only works when the larger class is up to 5x the size of the smaller one.\")\n",
    "        zeros = np.argwhere(blood_types == 0)\n",
    "        ones = np.argwhere(blood_types == 1)\n",
    "        \n",
    "        if len(zeros) > len(ones):\n",
    "            longer = zeros\n",
    "            shorter = ones\n",
    "        else:\n",
    "            longer = ones\n",
    "            shorter = zeros\n",
    "        keep_idx = np.concatenate((longer, shorter, shorter, shorter, shorter, shorter))\n",
    "        keep_idx = keep_idx[:len(longer) * 2]\n",
    "        np.random.shuffle(keep_idx)\n",
    "        blood_types = blood_types[keep_idx]\n",
    "        train_data = train_data[keep_idx]\n",
    "        \n",
    "        X = train_data.ravel().reshape(-1, train_data.shape[-1])\n",
    "        y = blood_types.ravel()\n",
    "    else:\n",
    "        print(\"Using no data augmentation\")\n",
    "        randomize_idx = np.arange(len(blood_types))\n",
    "        np.random.shuffle(randomize_idx)\n",
    "        X = train_data[randomize_idx,:]\n",
    "        y = blood_types[randomize_idx]\n",
    "    \n",
    "    tiledata = X\n",
    "    nnz = np.count_nonzero(tiledata,axis=0)\n",
    "\n",
    "    fracnnz = np.divide(nnz.astype(float),tiledata.shape[0])\n",
    "    \n",
    "    # Only keeping data that has less than 10% missing data\n",
    "    print(\"Removing columns with more than 10% missing data\")\n",
    "    idxKeep = fracnnz >= 0.9\n",
    "    tiledata = tiledata[:,idxKeep]\n",
    "\n",
    "    def foo(col):\n",
    "       u = np.unique(col)\n",
    "       nunq = u.shape\n",
    "       return nunq\n",
    "\n",
    "    invals = np.apply_along_axis(foo, 0, tiledata)\n",
    "    invals = invals[0]\n",
    "\n",
    "    varvals = np.full(50*tiledata.shape[1],np.nan)\n",
    "    nx=0\n",
    "\n",
    "    varlist = []\n",
    "    for j in range(0,tiledata.shape[1]):\n",
    "        u = np.unique(tiledata[:,j])\n",
    "        varvals[nx:nx+u.size] = u\n",
    "        nx = nx + u.size\n",
    "        varlist.append(u)\n",
    "\n",
    "    varvals = varvals[~np.isnan(varvals)]\n",
    "\n",
    "    # pathdataOH =  np.repeat(pathdata[idxKeep],invals)\n",
    "    # oldpath = np.repeat(idxOP[idxKeep],invals)\n",
    "\n",
    "    # Run the encoder in parts to determine rows that pass the signficance level (chi^2)\n",
    "\n",
    "    print(\"Beginning to one-hot encode data\")\n",
    "\n",
    "    ny = tiledata.shape[1]\n",
    "\n",
    "    nparts = 4\n",
    "\n",
    "    idx = np.linspace(0,ny,num=nparts).astype('int')\n",
    "\n",
    "    Xtrain2 = csr_matrix(np.empty([tiledata.shape[0], 0]))\n",
    "    pidx = np.empty([0,],dtype='bool')\n",
    "\n",
    "    for ichunk in np.arange(0,nparts-1):\n",
    "        imin = idx[ichunk]\n",
    "        imax = idx[ichunk+1]\n",
    "        enc = OneHotEncoder(sparse=True, dtype=np.uint16)\n",
    "\n",
    "        # 1-hot encoding tiled data\n",
    "        Xtrain = enc.fit_transform(tiledata[:,imin:imax])\n",
    "    \n",
    "        # print(pathdataOH.shape)\n",
    "        \n",
    "        if filter_type == 'chi2':\n",
    "            print(\"Using chi2 filter\")\n",
    "            [chi2val,pval] = chi2(Xtrain, y)\n",
    "            pidxchunk = pval <= 0.02\n",
    "            Xchunk = Xtrain[:,pidxchunk]\n",
    "            \n",
    "        else:\n",
    "            print(\"Using no filter\")\n",
    "            print(Xtrain.shape)\n",
    "            Xchunk = Xtrain\n",
    "            pidxchunk = np.ones(Xchunk.shape[1], dtype=bool)\n",
    "            \n",
    "        pidx=np.concatenate((pidx,pidxchunk),axis=0)\n",
    "        Xtrain2=hstack([Xtrain2,Xchunk],format='csr')\n",
    "\n",
    "    # pathdataOH = pathdataOH[pidx]\n",
    "    # oldpath = oldpath[pidx]\n",
    "    varvals = varvals[pidx]\n",
    "    Xtrain = Xtrain2\n",
    "    to_keep = varvals > 2 \n",
    "    idkTK = np.nonzero(to_keep)\n",
    "    idkTK = idkTK[0]\n",
    "\n",
    "    Xtrain = Xtrain[:,idkTK]\n",
    "    # pathdataOH = pathdataOH[idkTK]\n",
    "    # oldpath = oldpath[idkTK]\n",
    "    varvals = varvals[idkTK]\n",
    "    np.save(y_filename, y)\n",
    "    scipy.sparse.save_npz(X_filename, Xtrain)\n",
    "    print(\"Just created datasets for %s and %s\\n\" % (X_filename, y_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
